# global variables

#####
## PROXY
## proxy environment variable, mainly for fetching addons
#proxy_env:
#  http_proxy: 'http://proxy.corp.example.com:8080'
#  https_proxy: 'http://proxy.corp.example.com:8080'
#  no_proxy: '127.0.0.1,localhost,.example.com,.svc,.local,/var/run/docker.sock,.sock,sock'
#####

KUBERNETES_VERSION: "1.11.3"
# Software versions (used by installation via package manager)
#KUBERNETES_DNS_VERSION: "1.14.7"
#KUBERNETES_CNI_VERSION: "0.6.0"
KEEPALIVED_VERSION: "1.3.5"
  # Use "1.3.*" if you want the latest 1.3 provided by your already defined package repositories

#####
## PACKAGES (rpm/deb)
## Desired state for the yum packages (docker, kube*); it defaults to latest, trying to upgrade every time.
## package_state: latest # Other valid options for this context: installed
package_state: installed #latest #installed
## kube* requires full_kube_reinstallation set to True! (due to ansible (pre 2.4), which does not downgrade packages, you need to uninstalling first)
kubeadm_version: "{{ KUBERNETES_VERSION }}*" #1.9.* #1.7.* #8 # 1.6.11 # 1.8.1
kubelet_version: "{{ KUBERNETES_VERSION }}*" #1.9.* #1.7.* #8 # 1.6.11 # 1.8.1
kubectl_version: "{{ KUBERNETES_VERSION }}*" #1.9.* #1.7.* #8 # 1.6.11 # 1.8.1
# To find the possible versions, check: https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/filelists.xml
# curl -SL https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/filelists.xml | grep -A 1 'name="kubeadm' | grep ver
# curl -SL https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/filelists.xml | grep -A 1 'name="kubelet' | grep ver
# curl -SL https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/filelists.xml | grep -A 1 'name="kubectl' | grep ver
#####
##### NOTE ->> For the actual k8s version, please set the below: kubeadm_master_config.kubernetesVersion

## HA
CLUSTER_NAME: demo1  # used only for defining the below dnsDomain and masterha_fqdn

## ensure you have the DNS set for wildcard, and pointing all the trafic to master or similar setup
custom:
  networking:
    dnsDomain: "{{ CLUSTER_NAME }}.k8s.corp.example.com"  # For MasterHA, if you have dns, put the desired cluster domain here. If no DNS change possible on your side, and you want MasterHA, fix the below 2 values accordinly
    masterha_fqdn: "master-of-{{ CLUSTER_NAME }}.corp.example.com"  # Required when you have MasterHA, in order to set apiServerCertSANs correctly
    masterha_vip: "10.245.103.9" # Important when you have MasterHA
    #Check below if you want to have IPs replaced with FQDN - important if you use proxy !!!
    fqdn: 
    # decide where to force use fqdn for non masterha and nodes. 
    # When set to false, it will use the name as defined in the inventory file (unless specified differently)
      always: false # makes all the below true
      master: true  # when true, actions like wait/join will be done against dns name instead of IP
      node: false   # when true, the join command will have --node-name set to fqdn. When false, k8s will set based on how node machine answers to the hostname command
      etcd: false   # Relevant for MasterHA case(more than 1 node in the master group), ignored otherwise; When true, the etcd endpoints will be with fqdn, IP address otherwise
      
# Network settings
# Choices are: flannel, weavenet, calico
NETWORK_PLUGIN: flannel
POD_NETWORK_CIDR: 10.244.0.0/16

# Settings for the network plugins
CALICO_YAML_URL: "https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml"
WEAVENET_YAML_URL_PREFIX: "https://cloud.weave.works/k8s/net?k8s-version="
FLANNEL_YAML_URL: "https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml"

# Certificate settings (does not really matter for self-signed certificates in a private network)
CERT_COMMON_NAME: "etcd"
CERT_COUNTRY: "AA"
CERT_LOCALITY: "Locality"
CERT_ORGANISATION: "Example"
CERT_STATE: "State"
CERT_ORG_UNIT: "K8S"

## # Tags for some images
## COREDNS_TAG: "1.1.3"
## PAUSE_TAG: "3.1"
## ETCD_TAG: "3.2.18"
## NGINX_TAG: "1.14.0"
## ETCD_OPERATOR_TAG: "v0.9.2"
## CONFIGMAP_RELOAD_TAG: "v0.0.1"
## PROMETHEUS_OPERATOR_TAG: "v0.22.1"
## PROMETHEUS_TAG: "v2.3.1"
## BUSYBOX_TAG: "1.27.2"
## ELASTICSEARCH_TAG: "v5.6.4"
## ELASTICSEARCH_KUBERNETES_TAG: "6.2.3"
## CURATOR_TAG: "5.4.0"
## FLUENTD_ES_TAG: "v2.0.4"
## KIBANA_OSS_TAG: "6.2.2"
## DASHBOARD_TAG: "v1.8.3"
## #HEAPSTER_TAG: "v1.5.3"
## HEAPSTER_TAG: "v1.6.0-beta.1"
## HEAPSTER_GRAFANA_TAG: "v5.0.4"
## HEAPSTER_INFLUXDB_TAG: "v1.5.2"


########## Image pre-pull on MASTERS (only)
# Usually pre-pull not required, but there is this option here.
# Depending on what images you plan to use during deployment, you 
#####
# pre_pull_k8s_images: False
#####

####
## Docker images repo
## If you have your internal docker registry which proxies the external ones (e.g. a nexus3 with docker proxy to k8s.gcr.io and/or to docker.io), replace the value here:
# images_repo: "myk8sgcrio.corp.example.com" # k8s.gcr.io
#####

# Images for prefetching when pre_pull_k8s_images is true and images_repo is defined
HOST_ARCH: amd64
DOCKER_IMAGES: [
#  { name: "bobrik/curator", tag: "{{ CURATOR_TAG }}" },
#  { name: "{{images_repo}}/busybox", tag: "{{ BUSYBOX_TAG }}" },
#  #{ name: "dockerregistry.mylan.local:5000/fluentd-elasticsearch", tag: "{{ FLUENTD_ES_TAG }}" },
#  { name: "docker.elastic.co/kibana/kibana-oss", tag: "{{ KIBANA_OSS_TAG }}" },
#  { name: "{{images_repo}}/coredns", tag: "{{ COREDNS_TAG }}" },
#  { name: "{{images_repo}}/elasticsearch", tag: "{{ ELASTICSEARCH_TAG }}" }, #???
#  { name: "{{images_repo}}/etcd-{{ HOST_ARCH }}", tag: "{{ ETCD_TAG }}" },
#  { name: "{{images_repo}}/fluentd-elasticsearch", tag: "{{ FLUENTD_ES_TAG }}" },
#  { name: "{{images_repo}}/heapster-{{ HOST_ARCH }}", tag: "{{ HEAPSTER_TAG }}" },
#  { name: "{{images_repo}}/heapster-grafana-{{ HOST_ARCH }}", tag: "{{ HEAPSTER_GRAFANA_TAG }}" },
#  { name: "{{images_repo}}/heapster-influxdb-{{ HOST_ARCH }}", tag: "{{ HEAPSTER_INFLUXDB_TAG }}" },
#  { name: "{{images_repo}}/kube-apiserver-{{ HOST_ARCH }}", tag: "v{{ KUBERNETES_VERSION }}" },
#  { name: "{{images_repo}}/kube-controller-manager-{{ HOST_ARCH }}", tag: "v{{ KUBERNETES_VERSION }}" },
#  { name: "{{images_repo}}/kube-proxy-{{ HOST_ARCH }}", tag: "v{{ KUBERNETES_VERSION }}" },
#  { name: "{{images_repo}}/kube-scheduler-{{ HOST_ARCH }}", tag: "v{{ KUBERNETES_VERSION }}" },
#  { name: "{{images_repo}}/kubernetes-dashboard-{{ HOST_ARCH }}", tag: "{{ DASHBOARD_TAG }}" },
#  { name: "{{images_repo}}/pause-{{ HOST_ARCH }}", tag: "{{ PAUSE_TAG }}" },
#  { name: "{{images_repo}}/pause", tag: "{{ PAUSE_TAG }}" },
#  { name: "{{images_repo}}/nginx", tag: "{{ NGINX_TAG }}" },
#  { name: "{{images_repo}}/coreos/configmap-reload", tag: "{{ CONFIGMAP_RELOAD_TAG }}" },
#  { name: "{{images_repo}}/coreos/etcd-operator", tag: "{{ ETCD_OPERATOR_TAG }}" },
#  { name: "{{images_repo}}/coreos/etcd", tag: "v{{ ETCD_TAG }}" },
#  { name: "{{images_repo}}/coreos/prometheus-config-reloader", tag: "{{ PROMETHEUS_OPERATOR_TAG }}" },
#  { name: "{{images_repo}}/coreos/prometheus-operator", tag: "{{ PROMETHEUS_OPERATOR_TAG }}" },
#  { name: "{{images_repo}}/pires/docker-elasticsearch-kubernetes", tag: "{{ ELASTICSEARCH_KUBERNETES_TAG }}" },
#  { name: "{{images_repo}}/prometheus/prometheus", tag: "{{ PROMETHEUS_TAG }}" }
]

################################DONE HA

## When defined, it first forces uninstall any kube* packages (rpm/deb) from hosts
## When full_kube_reinstall is False or undefined, it will not reinstall # and it won't remove the /etc/kubernetes/ folder before reinstall
## Should be used only if a downgrade in kube* tools is required, otherwise it will waste a lot of time reinstalling unnecessary.
full_kube_reinstall: False # undefined is True. #Make it False for faster redeployments. Make true when you need downgrade (ansible does not downgrade rpms autoamtically)
etcd_clean: True # default is True, and it will cleanup /var/lib/etcd/ !
#####

#kubeadm_init_args: "--skip-preflight-checks" 
#kubeadm_join_args: "--skip-preflight-checks" #  For 1.8+ you may want to add: --discovery-token-unsafe-skip-ca-verification"
#--ignore-preflight-errors=all
kubeadm_join_args: "--discovery-token-unsafe-skip-ca-verification"
turn_swapoff: True #False #True #false # default true # Note: by default kubelet won't start if swap not disabled of or kubelet not instructed to accept swap on.

#####
## NTP SETUP
## it is mandatory to have the time from all machines in sync
ntp_setup: True  # Default True
## ntp does not work via proxy, so, if ntp cannot reach external servers, define here the internal ntp server:
#ntp_conf: |
#  server ntp1.corp.example.com
#  server pool.ntp.org
#####

#####
## hostname fix: set_hostname_to_inventory_hostname
## to make sure the hostname in inventory (usually fqdn) is in sync with the hostname as seen inside the host (usually required by vangrant)
#set_hostname_to_inventory_hostname: True #False # Default False
#####

kernel_modules_setup: True # default: True
#It will load the required kernel modules like ip_vs, bridge, nf_conntrack_ipv4, br_netfilter ;  echo 1 >/proc/sys/net/bridge/bridge-nf-call-iptables

#####
docker_setup: "auto" # when not defined, default is "auto"
  ## auto will install docker (if not yet installed), set it up (with overlay2 storage driver), (re)start it
  ## force will do the above even if it's already installed
  ## ignore will not check docker at all.
#####

#####
## Selinux
## If selinux_state is not defined, it will skip Selinux setup
## If values are defined, you may want to enable also "allow_restart"
#selinux_state: permissive #  OR: disabled # When undefined entire step is skipped
#selinux_policy: targeted  # defaults to targeted
#####

#####
## Allow restart (if required, e.g. if ansible's selinux module sets reboot_required to true )
allow_restart: True ## Default False
#####

#####
## Iptables
iptables_setup: True # Default is True. 
# This is not ideal or perfect! Review code and decide. It will disable&mask firewalld service (if exists), set default policies ACCEPT, and it will remove REJECT rules from all chains (INPUT,FW,OUT) with a commands like: iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited (for each). 
# If still issues, debug iptables on both master and nodes with: 
# watch -n1 iptables -vnL
# http://www.slsmk.com/how-to-log-iptables-dropped-packets-to-syslog/ and monitor with journalctl -kf
### # iptables_reset: False # False is default # Rarely used, this will be removed in the future
#####

#####
# reset_gracefully: False # False is default # Important if there was a cluster before and you want to shut it down using cordon and drain
#####

#####
## This is the configuration that will be used by kubeadm init on master.
## Structure comes from: https://kubernetes.io/docs/admin/kubeadm/#config-file
kubeadm_master_config:
  apiVersion: kubeadm.k8s.io/v1alpha2
  # this is updated to v1alpha2 , starting 1.11. # but we must keep cloudProvider section (due to an ansible limitation)
  kind: MasterConfiguration
  api:
#    advertiseAddress: <address|string>  # When MasterHA, this is set automatically by our code to custom.networking.master_fqdn & custom.networking.master_vip
#    advertiseAddress: '10x.x...x' # Even when there is no MasterHA, usually required when working with vagrant, as the default (eth0) address is the NAT...
#    controlPlaneEndpoint: # When MasterHA, this is set automatically by our code to custom.networking.master_vip
#    bindPort: <int>
#  etcd:  # When Master HA is defined, the etcd section is generated in the code
#    endpoints:
#    - <endpoint1|string>
#    - <endpoint2|string>
#    caFile: <path|string>
#    certFile: <path|string>
#    keyFile: <path|string>
#    dataDir: <path|string>
#    extraArgs:
#      <argument>: <value|string>
#      <argument>: <value|string>
#    image: <string>
#    serverCertSANs:
#    - <name1|string>
#    - <name2|string>
#    peerCertSANs:
#    - <name1|string>
#    - <name2|string>
  kubeProxy:
    config:
      mode: "ipvs"
      # Leave mode undefined or "" for the default, which usually is the old iptables method
#      mode: <value|string>
#      bindAddress: <address|string>
#      clusterCIDR: <cidr>
  networking:
#    dnsDomain: "demo1.cluster.local"  ## default is: "cluster.local"
#    serviceSubnet: <cidr>
    podSubnet: "{{ POD_NETWORK_CIDR }}"
    #podSubnet: '10.244.0.0/16'  # Exactly this one is required when Flannel network is used. If you other network solutions, commented out this entry.
    #podSubnet: '192.168.0.0/16'  # Exactly this one is required when Calico network is used. If you other network solutions, this entry can be commented out.
    #podSubnet: '10.32.0.0/12'  # Exactly this one is required when Weave network is used (with defaults). If you other network solutions, this entry can be commented out.
  kubernetesVersion: "v{{ KUBERNETES_VERSION }}" #'v1.10.2' #'v1.7.11' # If not defined, it will require internet access to find which is the latest one. kubeadm version has to be at least the requested k8s version.
                               # 1.7.12 # 1.6.12 # 1.8.8
  #cloudProvider: 'vsphere' # WE NEED THIS Even after 1.11 (v1alpha2) (due to a bug in ansible on vars with "-"); this is also required: govc vm.change -e="disk.enableUUID=1" -vm=<machines> and requires setup of cloud_config below
#  nodeName: <string>
#  authorizationModes:
#  - <authorizationMode1|string>
#  - <authorizationMode2|string>
  bootstrapTokens:
  - groups:
    - system:bootstrappers:kubeadm:default-node-token
    token: secret.token5yourbyok8s
    ttl: 24h0m0s
    usages:
    - signing
    - authentication
#  selfHosted: <bool>
  apiServerExtraArgs: # https://kubernetes.io/docs/admin/kube-apiserver/
    endpoint-reconciler-type: "lease"  # needs k8s 1.9+ More info: https://kubernetes.io/docs/admin/high-availability/building/#endpoint-reconciler
    service-node-port-range: '79-32767' #Default 32000-32767 ; Ensure the local ports on all nodes are set accordingly
  #  runtime-config: batch/v2alpha1  # Required if one wants to use batch jobs before 1.8
#    <argument>: <value|string>
#    kubelet-preferred-address-types: 'Hostname' #Default: [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]
#  controllerManagerExtraArgs: # https://kubernetes.io/docs/admin/kube-controller-manager/
#    pod-eviction-timeout: '3m00s' # Default 5m0s #PodEvictionTimeout controls grace peroid for deleting pods on failed nodes.  Takes time duration string (e.g. '300ms' or '2m30s').  Valid time units are 'ns', 'us', 'ms', 's', 'm', 'h'.
#    <argument>: <value|string>
#  schedulerExtraArgs:
#    <argument>: <value|string>
#    <argument>: <value|string>
#  apiServerExtraVolumes:
#  - name: <value|string>
#    hostPath: <value|string>
#    mountPath: <value|string>
#  controllerManagerExtraVolumes:
#  - name: <value|string>
#    hostPath: <value|string>
#    mountPath: <value|string>
#  schedulerExtraVolumes:
#  - name: <value|string>
#    hostPath: <value|string>
#    mountPath: <value|string>
#  apiServerCertSANs:  # Part of it is generated in the code. Even more is added when there is MasterHA
#     - 'kubernetes'
#    - <name1|string>
#    - <name2|string>
#  certificatesDir: <string>
#  imageRepository: <string>
#  unifiedControlPlaneImage: <string>
#  featureGates:
#    <feature>: <bool>
#    <feature>: <bool>
#####
## TAINTS (for master) & uncordon
## NoExecute evicts on the spot. (while NoSchedule does not allow new pods); other option: PreferNoSchedule
## FYI, by default, master has this taint: node-role.kubernetes.io/master:NoSchedule
## If you want to be able to schedule pods on the master, either set master_uncordon:true  (prefered option) or via taints section: uncomment 'node-role.kubernetes.io/master:NoSchedule-'
## It's useful if it's a single-machine Kubernetes cluster for development (replacing minikube)
## To see taints, use: kubectl describe nodes

#taints_master:
#- 'dedicated=master:NoExecute'                 # Force eviction of pods from master
#- 'dedicated=master:PreferNoSchedule'          # Safety net
#- 'dedicated:NoExecute-'                       # Puts the previous PreferNoSchedule into action - step1
#- 'node-role.kubernetes.io/master:NoSchedule-' # Puts the previous PreferNoSchedule into action - step2
#####

#####
## NETWORK
## it's not possible to have more than one network solution
## options: https://kubernetes.io/docs/admin/addons/
k8s_network_addons_urls:
## CALICO # For Calico one has to also ensure above setting kubeadm_master_config.networking.podSubnet is set to 192.168.0.0/16 )
#   - https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml # versions are 2.4,2.5,2.6
#     newer calico 3.0 :     
#   - https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml  
## OR Flanned: (for Flanned one has to also ensure above setting kubeadm_master_config.networking.podSubnet is set to 10.244.0.0/16 )
  ##- https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml
  #- https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml # For latest, replace v0.9.0 with master
  - https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml # For latest, replace v0.9.0 with master
# OR Weave: #https://www.weave.works/docs/net/latest/kubernetes/kube-addon/ 
# - https://cloud.weave.works/k8s/net?k8s-version={{kubeadm_master_config.kubernetesVersion}}&{{networking.podSubnet | default ('env.IPALLOC_RANGE=10.32.0.0/12') }}
#####

#####
# ADDONS # uncomment/ add the desired ones.
#k8s_addons_urls:
  # - https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
  # - https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/node-problem-detector/npd.yaml # rbac ready
  # - https://github.com/ReSearchITEng/kubeadm-playbook/raw/master/allow-all-all-rbac.yml # No longer required, as prods like nginx-ingress are now rbac ready!
#####

vsphere_storageclass_urls:
  - https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/storage-class/vsphere/default.yaml
  #- https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/vsphere/vsphere-volume-sc-fast.yaml

###################
## HELM & CHARTS ##
###################
helm:
  helm_version: v2.11.0 # or "latest" #https://github.com/kubernetes/helm/releases
  install_script_url: 'https://github.com/kubernetes/helm/raw/master/scripts/get' 
  repos: ## stable repo is installed by helm by default, no need for its entry here, add only new ones
    - { name: incubator, url: 'http://storage.googleapis.com/kubernetes-charts-incubator' }
#    - { name: rook, url: 'http://charts.rook.io' }
#    - { name: fabric8, url: 'https://fabric8.io/helm' }
  packages_list: # when not defined, namespace defaults to "default" namespace
    - { name: nginx-ingress, repo: stable/nginx-ingress, namespace: kube-system, options: '--set rbac.create=true --set controller.stats.enabled=true --set controller.service.type=NodePort --set controller.service.nodePorts.http=80 --set controller.service.nodePorts.https=443 --set controller.service.externalTrafficPolicy=Local --set controller.extraArgs.enable-ssl-passthrough="" --version=0.17.1 ' }
    - { name: heapster, repo: stable/heapster, namespace: kube-system, options: '--set service.nameOverride=heapster,rbac.create=true' }

## DASHBOARD HEAD(latest), compatible with k8s 1.8:
#    - { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,ingress.enabled=True,ingress.hosts[0]={{groups["master"][0]}},image.repository=kubernetesdashboarddev/kubernetes-dashboard-amd64,image.tag=head,image.pullPolicy=Always' } # development version, for k8s 1.8, and with all admin permissions open
# OR:
## DASHBOARD 1.8.3, compatible with k8s 1.8+:    
    - { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,rbac.clusterAdminRole=True,ingress.enabled=True,ingress.hosts[0]={{groups["master"][0]}},ingress.hosts[1]=dashboard.{{ custom.networking.dnsDomain }},image.tag=v1.8.3 --version=0.5.3' } # Limited to helmchart 0.5.3 for the non-tls ingress. 
    ##gcr.io/google_containers/kubernetes-dashboard-amd64 for stable versions tagged per k8s version. For lastest (beta) versions, one may use: from docker.io kubernetesdashboarddev/kubernetes-dashboard-amd64
    #- { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,ingress.enabled=True,ingress.hosts[0]={{groups["master"][0]}},ingress.hosts[1]=dashboard.{{ custom.networking.dnsDomain }},image=kubernetesdashboarddev/kubernetes-dashboard-amd64,imageTag=head,imagePullPolicy=Always' } # development version, and with all admin permissions open
    #- { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,ingress.enabled=True,ingress.hosts[0]={{groups["master"][0]}},ingress.hosts[1]=dashboard.{{ custom.networking.dnsDomain }},image.tag=v1.8.3 --version=0.5.3' }
#    - { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,ingress.enabled=True,ingress.hosts[0]={{groups["master"][0]}},ingress.hosts[1]=dashboard.{{ custom.networking.dnsDomain }},imageTag=v1.7.1' } # other vers: v1.6.3. All secure & stable prod version
#    - { name: dashboard, repo: stable/kubernetes-dashboard, options: '--set rbac.create=True,ingress.enabled=True,ingress.hosts[0]=dashboard.{{ custom.networking.dnsDomain }},imageTag=v1.7.1' } # other vers: v1.6.3. All secure & stable prod version
#    - { name: influxdb, repo: stable/influx, namespace:kube-system, options: '--set --set persistence.enabled=True' }
#    - { name: prometheus, repo: stable/prometheus, namespace:kube-system, options: '--set alertmanager.ingress.enabled=True,alertmanager.ingress.hosts[0]=alertmanager.{{custom.networking.dnsDomain}},server.ingress.enabled=True,server.ingress.hosts[0]=prometheus.{{custom.networking.dnsDomain}}'
#    - { name: moodle, repo: stable/moodle, options: '--set moodleUsername=admin,moodlePassword=password,mariadb.mariadbRootPassword=secretpassword,serviceType=ClusterIP' }

## Not tested:
#    - { name: elasticsearch, repo: incubator/elasticsearch }
#####


##################  STORAGE  ################
#############################################

## When reseting a previous instalaltion, should it first remove the exsting pvcs&pvs (default false)?
storage:
  delete_pvs: true

##### STORAGE OPTION: Self Created NFS ###
##########################################
## Creates a nfs server on the master and exports the below path from the master to all cluster
nfs_k8s: #https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/nfs/provisioner/nfs-server-gce-pv.yaml
         #https://github.com/kubernetes-incubator/nfs-provisioner
  #enabled: "true"
  enabled: False
  provisioner: nfs.k8s
  # Path on the master node:
  host_path: /storage/nfs
  is_default_class: 'true' # case sensitive! Also: only one class can be default. Note that vpshere thin is also trying to be set as default, choose which one you want as default
  wipe: true # When set to true, every reset the files under host_path will be wiped !!!

##### STORAGE OPTION: Rook (ceph) ########
##########################################
## Rook - Ceph Distributed Software Storage
## As per spec section of: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-cluster.yaml
rook:
  enabled: false
  os_packages:
  - jq
  reset:
    storage_delete: true
  ## OLD Installation type, using url. Now we use the helm chart which wraps it.
  #operator_url:
  #  https://github.com/rook/rook/raw/master/demo/kubernetes/rook-operator.yaml
  client_tools_url:
  - https://github.com/rook/rook/raw/master/demo/kubernetes/rook-client.yaml
  - https://github.com/rook/rook/raw/master/demo/kubernetes/rook-tools.yaml
  sharedfs:
    enabled: false
    fs:
    - { name: "sharedfs", replication: 2 } #ceph osd pool set sharedfs-data size 2 && ceph osd pool set sharedfs-metadata size 2
  allowed_consumer_namespaces:  #E.g.: kubectl get secret rook-admin -n rook -o json | jq '.metadata.namespace = "kube-system"' | kubectl apply -f - # as per: https://github.com/rook/rook/blob/master/Documentation/k8s-filesystem.md
  - "kube-system"
  - "default"
  cluster_spec: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-cluster.yaml and https://github.com/rook/rook/blob/master/Documentation/cluster-tpr.md
    versionTag: master-latest
    dataDirHostPath: /storage/rook
    storage:                # cluster level storage configuration and selection
      useAllNodes: true
      useAllDevices: false
      deviceFilter:
      metadataDevice:
      location:
      storeConfig:
        storeType: filestore
        databaseSizeMB: 1024 # this value can be removed for environments with normal sized disks (100 GB or larger)
        journalSizeMB: 1024  # this value can be removed for environments with normal sized disks (20 GB or larger)
  ## Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
  ## nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
  #    nodes:
  #    - name: "172.17.4.101"
  #     directories:         # specific directores to use for storage can be specified for each node
  #     - path: "/rook/storage-dir"
  #   - name: "172.17.4.201"
  #     devices:             # specific devices to use for storage can be specified for each node
  #     - name: "sdb"
  #     - name: "sdc"
  #     storeConfig:         # configuration can be specified at the node level which overrides the cluster level config
  #       storeType: bluestore
  #   - name: "172.17.4.301"
  #     deviceFilter: "^sd."

## ADVANCED rook options:
  rbd:
    enabled: true
    pool_spec: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-storageclass.yaml and https://github.com/rook/rook/blob/master/Documentation/pool-tpr.md
      replication:
        size: 1
      ## For an erasure-coded pool, comment out the replication size above and uncomment the following settings.
      ## Make sure you have enough OSDs to support the replica size or erasure code chunks.
      #erasureCode:
      #  codingChunks: 2
      #  dataChunks: 2

    storageclass_parameters: # as per: https://github.com/rook/rook/blob/master/demo/kubernetes/rook-storageclass.yaml
      pool: replicapool
      ## Specify the Rook cluster from which to create volumes. If not specified, it will use `rook` as the namespace and name of the cluster.
      # clusterName: rook
      # clusterNamespace: rook

  ##ceph_conf: as per https://github.com/rook/rook/blob/master/Documentation/advanced-configuration.md
  #ceph_conf: |
  #  [global]
  #  osd crush update on start = false
  #  osd pool default size = 2

  monitoring: # as per: https://github.com/rook/rook/blob/master/Documentation/k8s-monitoring.md
    enabled: true

#####

##### STORAGE OPTION: VMWARE VSPHERE Storage #
##############################################
##### Note: This requires the cloud provider setting above:
##### kubeadm_master_config.cloudProvider: 'vsphere'
#####
cloud_config: |
    [Global]
    ## Vsphere:
    ## One must ensure:
    ##  - all vms have this enabled: ./govc vm.change -e="disk.enableUUID=1" -vm=<machine1-x>
    ##  - all vms are in the same VCenter
    ##  - the user below has the following roles at vcenter level:
    ## Datastore > Allocate space
    ## Datastore > Low level file Operations
    ## Virtual Machine > Configuration > Add existing disk
    ## Virtual Machine > Configuration > Add or remove device
    ## Virtual Machine > Configuration > Remove disk
    ## Virtual machine > Configuration > Add new disk
    ## Virtual Machine > Inventory > Create new
    ## Network > Assign network
    ## Resource > Assign virtual machine to resource pool
    ## Profile-driven storage -> Profile-driven storage view
    [Global]
      user = user@corp.example.com
      password = PASSWORD
      server = vcenter.corp.example.com:443
      insecure-flag = 1
      datacenter = DC01
      datastore = DS01
      ##./govc vm.info -vm.dns=machine01 | grep Path #and remove the machine name (last string)
      ## Working dir is necessary when your machines are under a directory (and all have to be under the same one)
      # working-dir = kubernetes

      ## Setup of per machine vm-uuid is usually not required, and it's determined automatically.
      #cat /sys/class/dmi/id/product_serial   and format like: "4237558d-2231-78b9-e07e-e9028e7cf4a5"
      #or: ./govc vm.info -vm.dns=machine01 | grep UUID #(well formated also)
      #machine01: vm-uuid="4215e1de-26df-21ec-c79e-2105fe3f9ad1"
      #machine02: vm-uuid="4215f1e4-6abd-cff1-1a4c-71ec169d7b11"
    [Disk]
      #scsicontrollertype = lsilogic-sas
      scsicontrollertype = pvscsi

#####

########## VARIOUS GENERIC SETTINGS #####
## This will be removed in the future versions
# kubeadm_docker_insecure_registry: registry.example.com:5000
#####

#####
# VARIOUS
# shell for bash-completion for kubectl, kubeadm, helm; currently only bash and zsh is supported.
shell: 'bash'
#####

